{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "226d62d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 1+10\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "# print(model)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('D:/KJE_Airiss/cow_data_for_AI/all_dataset_rm_bg_cow_deform_no_rotate_with_json3(500x500)/save_model/learning_rate=0.005/epoch_new_1.pt')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c38984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AiRISS\\AppData\\Local\\Temp\\ipykernel_8732\\3895507037.py:169: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.as_tensor( masks, dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [   0/4130]  eta: 13:02:51  lr: 0.000010  loss: 6.0813 (6.0813)  loss_classifier: 3.2190 (3.2190)  loss_box_reg: 0.8293 (0.8293)  loss_mask: 1.3363 (1.3363)  loss_objectness: 0.6282 (0.6282)  loss_rpn_box_reg: 0.0686 (0.0686)  time: 11.3731  data: 4.1954  max mem: 2741\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 465>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    504\u001b[0m data_loader_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m    505\u001b[0m dataset_test, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    506\u001b[0m collate_fn\u001b[38;5;241m=\u001b[39mutils\u001b[38;5;241m.\u001b[39mcollate_fn)  \n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m##################################                    train                 #########################################\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;66;03m# train for one epoch, printing every 10 iterations\u001b[39;00m\n\u001b[1;32m--> 510\u001b[0m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    511\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/KJE_Airiss/cow_data_for_AI/all_dataset_rm_bg_cow_deform_no_rotate_with_json3_new_correct_23class(500x500)/save_model/epoch_all_cow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(epoch) \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m# update the learning rate\u001b[39;00m\n",
      "File \u001b[1;32m~\\Container_python_analysis\\vision\\references\\detection\\engine.py:27\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, data_loader, device, epoch, print_freq, scaler)\u001b[0m\n\u001b[0;32m     21\u001b[0m     warmup_iters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m1000\u001b[39m, \u001b[38;5;28mlen\u001b[39m(data_loader) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     23\u001b[0m     lr_scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mLinearLR(\n\u001b[0;32m     24\u001b[0m         optimizer, start_factor\u001b[38;5;241m=\u001b[39mwarmup_factor, total_iters\u001b[38;5;241m=\u001b[39mwarmup_iters\n\u001b[0;32m     25\u001b[0m     )\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m metric_logger\u001b[38;5;241m.\u001b[39mlog_every(data_loader, print_freq, header):\n\u001b[0;32m     28\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(image\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images)\n\u001b[0;32m     29\u001b[0m     targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n",
      "File \u001b[1;32m~\\Container_python_analysis\\utils.py:171\u001b[0m, in \u001b[0;36mMetricLogger.log_every\u001b[1;34m(self, iterable, print_freq, header)\u001b[0m\n\u001b[0;32m    167\u001b[0m     log_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelimiter\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m    168\u001b[0m         [header, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m space_fmt \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}/\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meta: \u001b[39m\u001b[38;5;132;01m{eta}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{meters}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime: \u001b[39m\u001b[38;5;132;01m{time}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata: \u001b[39m\u001b[38;5;132;01m{data}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    169\u001b[0m     )\n\u001b[0;32m    170\u001b[0m MB \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024.0\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024.0\u001b[39m\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m    172\u001b[0m     data_time\u001b[38;5;241m.\u001b[39mupdate(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m end)\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m obj\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataset.py:471\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[1;32m--> 471\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mCowDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;66;03m# 객체 종류는 한 종류만 존재합니다(역자주: 예제에서는 사람만이 대상입니다)\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m#         labels = torch.as_tensor(labels2, dtype=torch.int64)\u001b[39;00m\n\u001b[0;32m    168\u001b[0m         labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(categ, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\n\u001b[1;32m--> 169\u001b[0m         masks \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m         image_id \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor([image_id])\n\u001b[0;32m    172\u001b[0m         area \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor( area)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "mask_colors=[[1,1,0],[1,0,1],[0,0,1],[0,1,0],[1,0,0],[0,1,1],[0.5,0.5,0],[0.7,0.3,0.5],[1,0.5,0.1],[0.3,0.5,0.8]]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# root='C:/Users/AiRISS/container_sinsundae_resize/'\n",
    "def coordinate_segm(each_segm):\n",
    "            return [((each_segm[0][i],each_segm[0][i+1])) for i in range(0,len(each_segm[0]),2)]\n",
    "import torch\n",
    "import numpy\n",
    "from PIL import Image\n",
    "class CowDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root,imgs, transforms,pc_train_python):\n",
    "        \n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # 모든 이미지 파일들을 읽고, 정렬하여\n",
    "        # 이미지와 분할 마스크 정렬을 확인합니다\n",
    "#         self.imgs = list(sorted(os.listdir(os.path.join(root))))\n",
    "        self.imgs = imgs\n",
    "#         print(self.imgs)\n",
    "        idx1=0\n",
    "        annot_image_id=[]\n",
    "        for ii in pc_train_python['annotations']:\n",
    "\n",
    "            tmp=pc_train_python['annotations'][idx1]\n",
    "            idx1=idx1+1\n",
    "            annot_image_id.append(tmp['image_id'])  \n",
    "            \n",
    "        self.annot_image_id=annot_image_id\n",
    "        self.pc_train_python  =pc_train_python  \n",
    "\n",
    "        \n",
    "\n",
    "    ###########################################################################\n",
    "    def coordinate_segm(each_segm):\n",
    "            return [((each_segm[0][i],each_segm[0][i+1])) for i in range(0,len(each_segm[0]),2)]\n",
    "\n",
    "   \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # 이미지와 마스크를 읽어옵니다\n",
    "#         self.root = root\n",
    "# #         self.transforms = transforms\n",
    "#         # 모든 이미지 파일들을 읽고, 정렬하여\n",
    "#         # 이미지와 분할 마스크 정렬을 확인합니다\n",
    "#         self.imgs = list(sorted(os.listdir(os.path.join(root))))\n",
    "        \n",
    "        pc_train_python    = self.pc_train_python           \n",
    "        img_path = os.path.join(self.root, self.imgs[idx])\n",
    "#         print(img_path)\n",
    "        #         mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        #         img = cv2.imread('train/{}.png'.format(self.all_img_id[idx-1]),cv2.IMREAD_COLOR)\n",
    "#         img2 = imread(img_path)\n",
    "#         plt.figure\n",
    "#         plt.imshow(img2)\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        # 분할 마스크는 RGB로 변환하지 않음을 유의하세요\n",
    "        # 왜냐하면 각 색상은 다른 인스턴스에 해당하며, 0은 배경에 해당합니다\n",
    "        \n",
    "        for img_id in range(len(pc_train_python['images'])):\n",
    "            all_segm=[]\n",
    "            all_categ=[]\n",
    "            all_img_id=[]\n",
    "            all_bbox=[]\n",
    "            all_area=[]\n",
    "            all_iscrowd=[]\n",
    "            if pc_train_python['images'][img_id]['file_name']==self.imgs[idx]:\n",
    "#         for img_id in range((3)): \n",
    "                \n",
    "                img_id1=int(pc_train_python['images'][img_id]['id'])\n",
    "#                 print(img_id1)\n",
    "                idx_segm=[ i for i,img_id2 in enumerate(self.annot_image_id) if img_id2==img_id1]\n",
    "#                 print(idx_segm)\n",
    "                segm=[]\n",
    "                categ=[]\n",
    "                img_id_tmp=[]\n",
    "                boxes1=[]\n",
    "                area1=[]\n",
    "                all_img_id_bbox=[]\n",
    "                all_img_id_area=[] \n",
    "                iscrowd=[]\n",
    "                for ii in idx_segm:\n",
    "                #     print(ii)\n",
    "                    segm.append(pc_train_python['annotations'][ii][\"segmentation\"])\n",
    "                    categ.append(pc_train_python['annotations'][ii]['category_id'])\n",
    "                    tmp_bbox=pc_train_python['annotations'][ii]['bbox']\n",
    "\n",
    "                    tmp_bbox[2], tmp_bbox[3] = tmp_bbox[0]+tmp_bbox[2], tmp_bbox[3]+tmp_bbox[1]\n",
    "\n",
    "                    boxes1.append(tmp_bbox)\n",
    "                    area1.append(pc_train_python['annotations'][ii]['area'])\n",
    "                    img_id_tmp.append(pc_train_python['annotations'][ii]['image_id'])\n",
    "                    iscrowd.append(int(pc_train_python['annotations'][ii]['iscrowd']))\n",
    "\n",
    "#                 all_segm.append(segm)\n",
    "#                 all_categ.append(categ)\n",
    "#                 all_bbox.append(boxes1)\n",
    "#                 all_area.append(area1)\n",
    "#                 all_img_id.append(img_id1)\n",
    "#                 all_iscrowd.append(iscrowd)\n",
    "#                 print(len(segm))\n",
    "        \n",
    "        self.all_segm=segm\n",
    "        self.all_categ=categ    \n",
    "        self.all_bbox=boxes1\n",
    "        self.all_area=area1                      \n",
    "        self.all_img_id=img_id1\n",
    "        self.all_iscrowd=iscrowd \n",
    "####################################################################################################################################       \n",
    "        segm=self.all_segm  # [[segment][image_idx][segment_idx]]\n",
    "        categ=self.all_categ # [[category][image_idx][segment_idx]]\n",
    "\n",
    "        boxes=self.all_bbox\n",
    "#         print(boxes)\n",
    "        labels=self.all_categ\n",
    "        image_id=self.all_img_id\n",
    "        area=self.all_area\n",
    "        iscrowd=self.all_iscrowd\n",
    "#         print(img)\n",
    "        img2=numpy.asarray(img)\n",
    "\n",
    "        h,w,c= img2.shape\n",
    "\n",
    "        segm2=segm\n",
    "        mask=[]\n",
    "#         print('length segmentation=',len(segm2))\n",
    "        for ii in range(len(segm2)):\n",
    "            mask3=[]\n",
    "#             print(ii)\n",
    "            mask1 = np.zeros((h,w))\n",
    "            for dd in range(len(segm2[ii])):\n",
    "#             for dd in [0]:\n",
    "#                 \n",
    "                each_segm=   segm2[ii][dd]  \n",
    "                coodinate1=coordinate_segm([each_segm])\n",
    "                arr = np.array(coodinate1, np.int32)\n",
    "                mask1 = cv2.fillPoly(mask1, [arr], (1,1))\n",
    "#                 plt.figure()\n",
    "#                 plt.imshow(mask1)\n",
    "            mask3=mask1.tolist()\n",
    "\n",
    "            mask.append(mask3)\n",
    "        masks=mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         with open(self.imgs[idx]+'_masks_'+ '.pkl', 'wb') as f:\n",
    "#             pickle.dump(masks, f)\n",
    "\n",
    "\n",
    "#         print('box=',boxes)\n",
    "#         print('label=',categ)\n",
    "#         print('masks=',len(masks))\n",
    "\n",
    "#         print('image_id2=',image_id)\n",
    "#         print('area2=',area)\n",
    "#         print('iscrowd2=',iscrowd)\n",
    "        # 모든 것을 torch.Tensor 타입으로 변환합니다\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # 객체 종류는 한 종류만 존재합니다(역자주: 예제에서는 사람만이 대상입니다)\n",
    "#         labels = torch.as_tensor(labels2, dtype=torch.int64)\n",
    "\n",
    "        labels = torch.as_tensor(categ, dtype=torch.int64)\n",
    "        masks = torch.as_tensor( masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.as_tensor([image_id])\n",
    "        area = torch.as_tensor( area)\n",
    "        # 모든 인스턴스는 군중(crowd) 상태가 아님을 가정합니다\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)  \n",
    "    \n",
    "    \n",
    "# root='C:/Users/AiRISS/container_sinsundae_resize/'\n",
    "import torch\n",
    "import numpy\n",
    "from PIL import Image\n",
    "\n",
    "class CowDataset_test(torch.utils.data.Dataset):\n",
    "    def __init__(self, root,imgs, transforms,pc_train_python):\n",
    "        \n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # 모든 이미지 파일들을 읽고, 정렬하여\n",
    "        # 이미지와 분할 마스크 정렬을 확인합니다\n",
    "#         self.imgs = list(sorted(os.listdir(os.path.join(root))))\n",
    "        self.imgs = imgs\n",
    "#         print(self.imgs)\n",
    "        idx1=0\n",
    "        annot_image_id=[]\n",
    "        for ii in pc_train_python['annotations']:\n",
    "\n",
    "            tmp=pc_train_python['annotations'][idx1]\n",
    "            idx1=idx1+1\n",
    "            annot_image_id.append(tmp['image_id'])  \n",
    "            \n",
    "        self.annot_image_id=annot_image_id\n",
    "        self.pc_train_python=pc_train_python\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "    ###########################################################################\n",
    "    def coordinate_segm(each_segm):\n",
    "            return [((each_segm[0][i],each_segm[0][i+1])) for i in range(0,len(each_segm[0]),2)]\n",
    "\n",
    "   \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # 이미지와 마스크를 읽어옵니다\n",
    "#         self.root = root\n",
    "# #         self.transforms = transforms\n",
    "#         # 모든 이미지 파일들을 읽고, 정렬하여\n",
    "#         # 이미지와 분할 마스크 정렬을 확인합니다\n",
    "#         self.imgs = list(sorted(os.listdir(os.path.join(root))))\n",
    "        \n",
    "        pc_train_python    = self.pc_train_python      \n",
    "        img_path = os.path.join(self.root, self.imgs[idx])\n",
    "#         print(img_path)\n",
    "        #         mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        #         img = cv2.imread('train/{}.png'.format(self.all_img_id[idx-1]),cv2.IMREAD_COLOR)\n",
    "#         img2 = imread(img_path)\n",
    "#         plt.figure\n",
    "#         plt.imshow(img2)\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        # 분할 마스크는 RGB로 변환하지 않음을 유의하세요\n",
    "        # 왜냐하면 각 색상은 다른 인스턴스에 해당하며, 0은 배경에 해당합니다\n",
    "        segm=[]\n",
    "        categ=[]\n",
    "        img_id_tmp=[]\n",
    "        img_id1=[]\n",
    "        boxes1=[]\n",
    "        area1=[]\n",
    "        all_img_id_bbox=[]\n",
    "        all_img_id_area=[] \n",
    "        iscrowd=[]\n",
    "        \n",
    "#         print(len(pc_train_python['images']))\n",
    "        for img_id in range(len(pc_train_python['images'])):\n",
    "            all_segm=[]\n",
    "            all_categ=[]\n",
    "            all_img_id=[]\n",
    "            all_bbox=[]\n",
    "            all_area=[]\n",
    "            all_iscrowd=[]\n",
    "            if pc_train_python['images'][img_id]['file_name']==self.imgs[idx]:\n",
    "#         for img_id in range((3)): \n",
    "                \n",
    "                img_id1=int(pc_train_python['images'][img_id]['id'])\n",
    "#                 print(img_id1)\n",
    "                idx_segm=[ i for i,img_id2 in enumerate(self.annot_image_id) if img_id2==img_id1]\n",
    "#                 print(idx_segm)\n",
    "                segm=[]\n",
    "                categ=[]\n",
    "                img_id_tmp=[]\n",
    "                boxes1=[]\n",
    "                area1=[]\n",
    "                all_img_id_bbox=[]\n",
    "                all_img_id_area=[] \n",
    "                iscrowd=[]\n",
    "                for ii in idx_segm:\n",
    "                #     print(ii)\n",
    "                    segm.append(pc_train_python['annotations'][ii][\"segmentation\"])\n",
    "                    categ.append(pc_train_python['annotations'][ii]['category_id'])\n",
    "                    tmp_bbox=pc_train_python['annotations'][ii]['bbox']\n",
    "\n",
    "                    tmp_bbox[2], tmp_bbox[3] = tmp_bbox[0]+tmp_bbox[2], tmp_bbox[3]+tmp_bbox[1]\n",
    "\n",
    "                    boxes1.append(tmp_bbox)\n",
    "                    area1.append(pc_train_python['annotations'][ii]['area'])\n",
    "                    img_id_tmp.append(pc_train_python['annotations'][ii]['image_id'])\n",
    "                    iscrowd.append(int(pc_train_python['annotations'][ii]['iscrowd']))\n",
    "\n",
    "#                 all_segm.append(segm)\n",
    "#                 all_categ.append(categ)\n",
    "#                 all_bbox.append(boxes1)\n",
    "#                 all_area.append(area1)\n",
    "#                 all_img_id.append(img_id1)\n",
    "#                 all_iscrowd.append(iscrowd)\n",
    "#                 print(len(segm))\n",
    "        \n",
    "        self.all_segm=segm\n",
    "        self.all_categ=categ    \n",
    "        self.all_bbox=boxes1\n",
    "        self.all_area=area1                      \n",
    "        self.all_img_id=img_id_tmp\n",
    "        self.all_iscrowd=iscrowd \n",
    "####################################################################################################################################       \n",
    "        segm=self.all_segm  # [[segment][image_idx][segment_idx]]\n",
    "        categ=self.all_categ # [[category][image_idx][segment_idx]]\n",
    "\n",
    "        boxes=self.all_bbox\n",
    "#         print(boxes)\n",
    "        labels=self.all_categ\n",
    "        image_id=self.all_img_id\n",
    "        area=self.all_area\n",
    "        iscrowd=self.all_iscrowd\n",
    "#         print(img)\n",
    "        img2=numpy.asarray(img)\n",
    "\n",
    "        h,w,c= img2.shape\n",
    "\n",
    "        segm2=segm\n",
    "        mask=[]\n",
    "#         print('length segmentation=',len(segm2))\n",
    "        for ii in range(len(segm2)):\n",
    "            mask3=[]\n",
    "#             print(ii)\n",
    "            mask1 = np.zeros((h,w))\n",
    "            for dd in range(len(segm2[ii])):\n",
    "#             for dd in [0]:\n",
    "#                 \n",
    "                each_segm=   segm2[ii][dd]  \n",
    "                coodinate1=coordinate_segm([each_segm])\n",
    "                arr = np.array(coodinate1, np.int32)\n",
    "                mask1 = cv2.fillPoly(mask1, [arr], (1,1))\n",
    "#                 plt.figure()\n",
    "#                 plt.imshow(mask1)\n",
    "            mask3=mask1.tolist()\n",
    "\n",
    "            mask.append(mask3)\n",
    "        masks=mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         with open(self.imgs[idx]+'_masks_'+ '.pkl', 'wb') as f:\n",
    "#             pickle.dump(masks, f)\n",
    "        image_id=np.unique(image_id)\n",
    "\n",
    "#         print('box=',boxes)\n",
    "#         print('label=',categ)\n",
    "#         print('masks=',len(masks))\n",
    "\n",
    "#         print('image_id2=',image_id)\n",
    "#         print('area2=',area)\n",
    "#         print('iscrowd2=',iscrowd)\n",
    "        # 모든 것을 torch.Tensor 타입으로 변환합니다\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # 객체 종류는 한 종류만 존재합니다(역자주: 예제에서는 사람만이 대상입니다)\n",
    "#         labels = torch.as_tensor(labels2, dtype=torch.int64)\n",
    "\n",
    "        labels = torch.as_tensor(categ, dtype=torch.int64)\n",
    "        masks = torch.as_tensor( masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.as_tensor([image_id])\n",
    "        area = torch.as_tensor( area)\n",
    "        # 모든 인스턴스는 군중(crowd) 상태가 아님을 가정합니다\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)  \n",
    "    \n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    # COCO 에서 미리 학습된 인스턴스 분할 모델을 읽어옵니다\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # 분류를 위한 입력 특징 차원을 얻습니다\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # 미리 학습된 헤더를 새로운 것으로 바꿉니다\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # 마스크 분류기를 위한 입력 특징들의 차원을 얻습니다\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # 마스크 예측기를 새로운 것으로 바꿉니다\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "import os\n",
    "os.chdir('C:/Users/AiRISS/Container_python_analysis')\n",
    "from vision.references.detection.engine import train_one_epoch, evaluate\n",
    "import vision.references.detection.utils as utils\n",
    "import vision.references.detection.transforms as T\n",
    "\n",
    "from torchvision import transforms as T\n",
    "import transforms as T\n",
    "os.chdir('C:/Users/AiRISS')\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # (역자주: 학습시 50% 확률로 학습 영상을 좌우 반전 변환합니다)\n",
    "        \n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# root='C:/Users/AiRISS/yolact-container_bg_boxing/data/container_sinsundae_resize_gray_scale'\n",
    "# root='C:\\Users\\AiRISS\\yolact-container_bg_boxing\\data\\container_sinsundae_resize_gray_scale'\n",
    "# use our dataset and defined transformations\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 1+23\n",
    "\n",
    "# # get the model using our helper function\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "# # # print(model)\n",
    "# # # move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,momentum=0.9)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n",
    "\n",
    "num_classes = 1+23\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "num_epochs = 40\n",
    "import numpy as np\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ################################                 load dataset                 ########################################\n",
    "    with open(\"D:/KJE_Airiss/cow_data_for_AI/all_dataset_rm_bg_cow_deform_no_rotate_with_json3_new_correct_23class(500x500)/cow_json_train.json\",\"r\") as pc_train_json:  # pc : poice\n",
    "            pc_train_python = json.load(pc_train_json)\n",
    "    imgs=[]    \n",
    "    for kk in range(len(pc_train_python['images'])):\n",
    "        imgs.append(pc_train_python['images'][kk]['file_name'])\n",
    "    root='D:/KJE_Airiss/cow_data_for_AI/all_dataset_rm_bg_cow_deform_no_rotate_with_json3_new_correct_23class(500x500)/train'\n",
    "    dataset = CowDataset(root, imgs,get_transform(train=False),pc_train_python)\n",
    "\n",
    "\n",
    "    with open(\"D:/KJE_Airiss/cow_data_for_AI/all_dataset_rm_bg_cow_deform_no_rotate_with_json3_new_correct_23class(500x500)/cow_json_valid.json\",\"r\") as pc_train_json:  # pc : poice\n",
    "                pc_valid_python = json.load(pc_train_json)\n",
    "    imgs_vals=[]    \n",
    "    for kk in range(len(pc_valid_python['images'])):\n",
    "        imgs_vals.append(pc_valid_python['images'][kk]['file_name'])\n",
    "    root='D:/KJE_Airiss/cow_data_for_AI/all_dataset_rm_bg_cow_deform_no_rotate_with_json3_new_correct_23class(500x500)/valid'\n",
    "    dataset_test = CowDataset_test(root, imgs_vals,get_transform(train=False),pc_valid_python)\n",
    "\n",
    "    # split the dataset in train and test set\n",
    "    # torch.manual_seed(1)\n",
    "    indices = torch.randperm(len(dataset)).tolist()\n",
    "\n",
    "    # indices = np.linspace(0,len(dataset)-1,len(dataset))\n",
    "    # indices=list(indices)\n",
    "    # indices = [int(item) for item in indices]\n",
    "    # print(indices)\n",
    "    dataset = torch.utils.data.Subset(dataset, indices)\n",
    "    # dataset = torch.utils.data.Subset(dataset,indices )\n",
    "    indices = torch.randperm(len(dataset_test)).tolist()\n",
    "    indices = np.linspace(0,len(dataset_test)-1,len(dataset_test))\n",
    "    indices=list(indices)\n",
    "    indices = [int(item) for item in indices]\n",
    "    dataset_test = torch.utils.data.Subset(dataset_test, indices)\n",
    "\n",
    "    # define training and validation data loaders\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=3, shuffle=True, num_workers=0,\n",
    "        collate_fn=utils.collate_fn)  # 원래는 num_workers=4 인데 이렇게 하면 에러남.\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=0,\n",
    "    collate_fn=utils.collate_fn)  \n",
    "    \n",
    "    ##################################                    train                 #########################################\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    torch.save(model.state_dict(), 'D:/KJE_Airiss/cow_data_for_AI/all_dataset_rm_bg_cow_deform_no_rotate_with_json3_new_correct_23class(500x500)/save_model/epoch_all_cow'+str(epoch) +'.pt')\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test,device='cuda')\n",
    "#     print(eval1.results1)\n",
    "#     model.eval()\n",
    "#     x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "#     predictions = model(x)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f66df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=[{'id': 1,\n",
    "  'name': '가시근',\n",
    "  'supercategory': 'mucle',\n",
    "  'color': '040439',\n",
    "  'metadata': 'korean_beef'},\n",
    " {'id': 2,\n",
    "  'name': '반가시근',\n",
    "  'supercategory': 'mucle',\n",
    "  'color': 'ba0e79',\n",
    "  'metadata': 'korean_beef'},\n",
    " {'id': 3,\n",
    "  'name': '널판근',\n",
    "  'supercategory': 'mucle',\n",
    "  'color': 'a6c2d0',\n",
    "  'metadata': 'korean_beef'},\n",
    " {'id': 4,\n",
    "  'name': '마름모근',\n",
    "  'supercategory': 'mucle',\n",
    "  'color': '542bb5',\n",
    "  'metadata': 'korean_beef'},\n",
    " {'id': 5,\n",
    "  'name': '등세모근',\n",
    "  'supercategory': 'mucle',\n",
    "  'color': '85a757',\n",
    "  'metadata': 'korean_beef'},\n",
    " {'id': 6,\n",
    "  'name': '배쪽톱니근',\n",
    "  'supercategory': 'mucle',\n",
    "  'color': '3b0411',\n",
    "  'metadata': 'korean_beef'},\n",
    " {'id': 7,\n",
    "  'name': '인대',\n",
    "  'supercategory': 'mucle',\n",
    "  'color': '628b89',\n",
    "  'metadata': 'korean_beef'},\n",
    " {'id': 8,\n",
    "  'name': '지방',\n",
    "  'supercategory': 'mucle',\n",
    "  'color': '68ab5b',\n",
    "  'metadata': 'korean_beef'},\n",
    " {'id': 9,\n",
    "  'name': '기타덧살',\n",
    "  'supercategory': 'mucle',\n",
    "  'color': 'd0534a',\n",
    "  'metadata': 'korean_beef'},\n",
    " {'id': 10,\n",
    "  'name': '최장근',\n",
    "  'supercategory': 'mucle',\n",
    "  'color': '609899',\n",
    "  'metadata': 'korean_beef'},\n",
    "{'id': 11,\n",
    "  'name': 'bone01',\n",
    "  'supercategory': 'bone',\n",
    "  'color': '040439',\n",
    "  'metadata': 'korean_beef'},\n",
    " {'id': 12,\n",
    "  'name': 'bone02',\n",
    "  'supercategory': 'bone',\n",
    "  'color': 'ba0e79',\n",
    "  'metadata': 'korean_beef'},\n",
    " {'id': 13,\n",
    "  'name': 'bone03',\n",
    "  'supercategory': 'bone',\n",
    "  'color': 'a6c2d0',\n",
    "  'metadata': 'korean_beef'},\n",
    " {'id': 14,\n",
    "  'name': 'bone04',\n",
    "  'supercategory': 'bone',\n",
    "  'color': '542bb5',\n",
    "  'metadata': 'korean_beef'},\n",
    " {'id': 15,\n",
    "  'name': 'bone05',\n",
    "  'supercategory': 'bone',\n",
    "  'color': '85a757',\n",
    "  'metadata': 'korean_beef'},\n",
    " {'id': 16,\n",
    "  'name': 'bone06',\n",
    "  'supercategory': 'bone',\n",
    "  'color': '3b0411',\n",
    "  'metadata': 'korean_beef'},\n",
    " {'id': 17,\n",
    "  'name': 'bone07',\n",
    "  'supercategory': 'bone',\n",
    "  'color': '628b89',\n",
    "  'metadata': 'korean_beef'},\n",
    " {'id': 18,\n",
    "  'name': 'bone08',\n",
    "  'supercategory': 'bone',\n",
    "  'color': '68ab5b',\n",
    "  'metadata': 'korean_beef'},\n",
    " {'id': 19,\n",
    "  'name': 'bone09',\n",
    "  'supercategory': 'bone',\n",
    "  'color': 'd0534a',\n",
    "  'metadata': 'korean_beef'},\n",
    " {'id': 20,\n",
    "  'name': 'bone10',\n",
    "  'supercategory': 'bone',\n",
    "  'color': '609899',\n",
    "  'metadata': 'korean_beef'},\n",
    "  {'id': 21,\n",
    "  'name': 'bone11',\n",
    "  'supercategory': 'bone',\n",
    "  'color': '68ab5b',\n",
    "  'metadata': 'korean_beef'},\n",
    " {'id': 22,\n",
    "  'name': 'bone12',\n",
    "  'supercategory': 'bone',\n",
    "  'color': 'd0534a',\n",
    "  'metadata': 'korean_beef'},\n",
    " {'id': 23,\n",
    "  'name': 'bone13',\n",
    "  'supercategory': 'bone',\n",
    "  'color': '609899',\n",
    "  'metadata': 'korean_beef'}]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "817bdc133b3334f6673937861dbf3c0b26835613222935352724746817bcb3cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
